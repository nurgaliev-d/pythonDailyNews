import g4f
from newsapi import NewsApiClient
from datetime import datetime, timedelta
import time
from dotenv import load_dotenv
import os

load_dotenv()
API_KEY = os.getenv("API_KEY")
newsapi = NewsApiClient(api_key=API_KEY)

topics = ["–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Ä–µ–∫–ª–∞–º–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏"]

def filter_advertisements(response_text):
    unwanted_phrases = [
        "Generated by BLACKBOX.AI",
        "try unlimited chat https://www.blackbox.ai"
    ]
    for phrase in unwanted_phrases:
        response_text = response_text.replace(phrase, "")
    return response_text.strip(", \n")

def get_fresh_news():
    week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    news_items = []

    for topic in topics:
        articles = newsapi.get_everything(
            q=topic,
            language='ru',
            from_param=week_ago,
            sort_by='publishedAt',
            page_size=2
        )['articles']

        for article in articles:
            if article['title'] and article['url']:
                news_items.append({
                    "title": article['title'],
                    "description": article['description'] or "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è",
                    "date": article['publishedAt'][:10],
                    "source": article['source']['name'],
                    "url": article['url']
                })
    return news_items

def summarize_with_g4f(news_list):
    combined_text = "\n\n".join([
        f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {n['title']}\n–û–ø–∏—Å–∞–Ω–∏–µ: {n['description']}\n–î–∞—Ç–∞: {n['date']}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {n['source']}\n–°—Å—ã–ª–∫–∞: {n['url']}"
        for n in news_list
    ])

    messages = [
        {
            "role": "system",
            "content": (
                "–¢—ã ‚Äî –ò–ò-–∂—É—Ä–Ω–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–∏—à–µ—Ç –∫—Ä–∞—Ç–∫–æ, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ –∏ –ø–æ –¥–µ–ª—É. "
                "–û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∫—Ä–∞—Å–∏–≤–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ, —Å —ç–º–æ–¥–∑–∏ –∏ –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π. "
                "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–∞–Ω–Ω—ã–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –¥–∞–Ω–æ."
            )
        },
        {
            "role": "user",
            "content": f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω—É—é –ø–æ–¥–±–æ—Ä–∫—É (5‚Äì7 –Ω–æ–≤–æ—Å—Ç–µ–π) –∏–∑ —Ç–µ–∫—Å—Ç–∞:\n\n{combined_text}"
        }
    ]

    response = g4f.ChatCompletion.create(
        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
        provider=g4f.Provider.DeepInfra,
        messages=messages,
        stream=False
    )

    return filter_advertisements(response)

def save_to_file(content):
    filename = "news_report.txt"
    with open(filename, "w", encoding="utf-8") as file:
        file.write(content)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ —Ñ–∞–π–ª: {filename}")

if __name__ == "__main__":
    print("üì∞ –ó–∞–≥—Ä—É–∂–∞—é —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é...\n")
    fresh_news = get_fresh_news()

    if not fresh_news:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –ü—Ä–æ–≤–µ—Ä—å API –∫–ª—é—á –∏–ª–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.")
    else:
        print("üß† –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é g4f...\n")
        summary = summarize_with_g4f(fresh_news)

        print("‚ú® –ü–æ–¥–±–æ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:\n")
        print(summary)

        save_to_file(summary)
        print("\nüìÖ –ö–æ–Ω–µ—Ü –æ—Ç—á—ë—Ç–∞.\n")
